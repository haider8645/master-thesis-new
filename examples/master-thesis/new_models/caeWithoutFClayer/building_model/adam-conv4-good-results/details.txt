#15/03/2018 : making the conv4 which gave good results to a smaller network of 128-64-32-16 since 256-128-64-32/10 already give respectable 
results but with alot of dead zones.

#15/03/2018: making the above network even smaller to 64-32-16-8, the above network gave good results for clustering
#16/03/2018: making the above network even smaler to 32-16-8-4, with relu


#17/03/2018: trained 32-16-8-4 with elu
#17/03/2018: trained 128-64-32-16-16-32-64-128 with ELU


#17/03/2018: trained: train above network with PReLU

#18/03/2018: trainied above network with a activation layer in the deconv0
#18/03/2018: trained the 16 feature maps conv4 network with kipro data with activation layer at deconv0
#18/03/2018: trained the 32 feature maps conv4 with kipro data with activation at deconv0

#19/03/2018: both 16 and 32 with activation at deconv0 gave poor results for kipro
#19/03/2018: 32 feature maps + prelu error goes up to 1e+08 range
#19/03/2018: trained  32 feature maps with relu, horrible results

#19/03/2018: trained 32 feature maps with relu @ batch 10 with relu, no activation deconv0, failure

#20/03/2018: trained 32 feature maps with new dataset with just 1 cropped center image of kipro dataset
base_lr 0.0001 and not 0.001

#25/03/2018: trained 32 feature maps with kipro, decon0 activated, prelu, xavier activation
#25/03/2018: trained 32 maps with kipro, deconv0 activated, msra weight filler, prelu
#16/04/2018: trained 32 maps with kipro, deconv0 activated, msra weights, prelu and train,test data from kipro
#20/04/2018: stopped training of the same above network without normalizing the dataset(e+11 range of errors .. )
#20/04/2018: training above network with relu and msra weight fillerx
#03/05/2018: training the above network with batch normalization and scale layers on hedrik prepared 3x227x227 dataset
#09/05/2018: training a new network for nir/img data with 2 loss layers
#10/05/2018: removing the normalizing and scale layer until i have a denormalizing and de-scale layer in caffe
#10/05/2018: poor reconstruction of above network, need a custom layer that does the reverse of BN and scale layer
#11/05/2018: adding a new fc layer after concat layer 
#12/05/2018: increased the nir and img fc layers to 900 each, summing to 1800 of total concat, respectable results of clusters
#12/05/2018: trained just for nir data (excellent clusters)
#12/05/2018: training just img data without scale layer
